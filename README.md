# Bananas - Distributed Task Queue

A distributed task queue system built with Go, Redis, and Docker.

## Description

Bananas is a lightweight, scalable distributed task queue system designed to handle asynchronous job processing across multiple workers. It uses Redis as the message broker and provides a simple API for job submission and management.

## ðŸ“– Documentation

- **[Integration Guide](./INTEGRATION_GUIDE.md)** - Complete guide for integrating Bananas into your projects
- **[Architecture & Components](./README.md#architecture)** - System design and component overview  
- **[API Documentation](./cmd/README.md)** - Service entry points (API, Worker, Scheduler)
- **[Internal Packages](./internal/README.md)** - Core packages (Config, Job, Queue, Worker)
- **[Client SDK](./pkg/README.md)** - Public client library documentation
- **[Tests](./tests/README.md)** - Integration test documentation

**New to Bananas?** Start with the [Integration Guide](./INTEGRATION_GUIDE.md) to learn how to add Bananas to your existing infrastructure.

## Getting Started

### Prerequisites

- Docker and Docker Compose

### Quick Start with Docker

There are two ways to run the system with Docker:

1. **Development mode** (with hot reload) - for active development
2. **Production mode** (optimized builds) - for testing production builds

## Development Mode (Recommended for Development)

Use the development setup for fast iteration with hot reload. Code changes are detected automatically and services restart instantly without rebuilding containers.

### Start Development Environment

```bash
make dev
# Or directly:
docker compose -f docker-compose.dev.yml up
```

This will start all services with:
- **Hot reload enabled** - code changes trigger automatic restarts
- **Source code mounted** - edit files on your host, changes reflect immediately
- **Fast feedback loop** - no container rebuilds needed

When you change any `.go` file:
- Only the affected service restarts (not all containers)
- Restart happens in ~1 second
- No Docker image rebuild required

### Stop Development Environment

```bash
make dev-down
# Or directly:
docker compose -f docker-compose.dev.yml down
```

## Production Mode

Use production mode to test optimized, production-ready builds.

### Build and Start All Services

```bash
make prod-build
# Or directly:
docker compose up --build
```

This will start:
- Redis server
- API server (accessible at http://localhost:8080)
- 3 worker instances (optimized binaries)
- 1 scheduler instance (optimized binaries)

### Scaling Workers

You can easily scale the number of workers:

```bash
make scale-workers N=5
# Or directly:
docker compose up --scale worker=5
```

### View Service Logs

View logs for a specific service:

```bash
# View worker logs
make logs-worker

# View API logs
make logs-api

# View all logs
make dev-logs
```

### Stop All Services

```bash
make down
# Or directly:
docker compose down
```

### Rebuild Production Images

After code changes, rebuild the optimized production images:

```bash
make prod-build
# Or directly:
docker compose up --build
```

### Run Individual Services

You can run individual services in either mode:

```bash
# Development mode - with hot reload
docker compose -f docker-compose.dev.yml up api

# Production mode - optimized builds
docker compose up api
```

### Available Make Commands

Run `make help` or just `make` to see all available commands:

```bash
make help
```

Quick reference:
- `make dev` - Start development environment
- `make prod-build` - Build and start production
- `make test` - Run all tests
- `make test-coverage` - Run tests with coverage report
- `make logs-worker` - View worker logs
- `make scale-workers N=5` - Scale workers
- `make clean` - Clean up everything

## Testing

### Running Tests

The project includes comprehensive test coverage (93.3%) across all components.

**Recommended: Run tests in Docker (works with any local Go version):**

```bash
# Run all tests in Docker container with Go 1.23
make test

# Run tests with verbose output in Docker
make test-verbose

# Generate coverage report in Docker
make test-coverage
```

**Alternative: Run tests locally (requires Go 1.21+):**

```bash
# Run tests on your local machine
make test-local

# Run tests with verbose output locally
make test-local-verbose

# Generate coverage report locally
make test-local-coverage
```

**Note**: The Docker-based test commands are recommended because:
- No local Go installation required
- Always uses the correct Go version (1.23)
- Consistent test environment
- Works on any machine with Docker

### Test Structure

- **Unit Tests**: Test individual components in isolation
  - `internal/job/types_test.go` - Job model tests (100% coverage)
  - `internal/worker/executor_test.go` - Job execution tests
  - `internal/worker/handler_test.go` - Handler registry tests
  - `internal/queue/redis_test.go` - Redis queue operations
  - `pkg/client/client_test.go` - Client SDK tests (95.2% coverage)

- **Integration Tests**: End-to-end workflow tests
  - `tests/integration_test.go` - Full system integration tests

## Redis Queue Implementation

Bananas uses Redis as its message broker with the following queue structure:

### Queue Keys

- **Job Storage**: `bananas:job:{id}` - Job data stored as JSON
- **Priority Queues**: 
  - `bananas:queue:high` - High priority jobs
  - `bananas:queue:normal` - Normal priority jobs
  - `bananas:queue:low` - Low priority jobs
- **Processing Queue**: `bananas:queue:processing` - Currently processing jobs
- **Dead Letter Queue**: `bananas:queue:dead` - Failed jobs (max retries exceeded)
- **Scheduled Set**: `bananas:queue:scheduled` - Jobs scheduled for future execution

### Queue Operations

**Enqueue**: Atomically stores job data and adds job ID to priority queue
```go
queue.Enqueue(ctx, job)
```

**Dequeue**: Atomically moves job from priority queue to processing queue (RPOPLPUSH)
```go
job, err := queue.Dequeue(ctx, []job.JobPriority{
    job.PriorityHigh,
    job.PriorityNormal,
    job.PriorityLow,
})
```

**Complete**: Updates job status and removes from processing queue
```go
queue.Complete(ctx, jobID)
```

**Fail**: Handles failures with exponential backoff retry logic
- If attempts < maxRetries: Schedules retry with exponential backoff (2^attempts seconds)
- If attempts >= maxRetries: Moves to dead letter queue
```go
queue.Fail(ctx, job, "error message")
```

**MoveScheduledToReady**: Moves ready jobs from scheduled set to priority queues
- Should be called periodically by scheduler (e.g., every second)
- Moves jobs whose retry time has arrived back to their priority queues
```go
count, err := queue.MoveScheduledToReady(ctx)
```

### Retry Strategy with Exponential Backoff

Failed jobs are not immediately re-enqueued. Instead, they're scheduled with exponential backoff:

- **1st retry**: 2 seconds delay (2^1)
- **2nd retry**: 4 seconds delay (2^2)
- **3rd retry**: 8 seconds delay (2^3)
- **Nth retry**: 2^N seconds delay

This approach prevents:
- **Thundering herd problems** when external services fail
- **Overwhelming failing dependencies** with retry storms
- **Priority queue pollution** with repeatedly failing jobs

Jobs awaiting retry are stored in a Redis sorted set (`bananas:queue:scheduled`) with their retry timestamp as the score. A background scheduler process periodically calls `MoveScheduledToReady()` to move ready jobs back to their priority queues.

### Features

**Queue Management:**
- âœ… Priority-based job processing (High > Normal > Low)
- âœ… Automatic retry with exponential backoff
- âœ… Scheduled set for delayed retry execution
- âœ… Dead letter queue for permanently failed jobs
- âœ… Atomic operations to prevent job loss
- âœ… Thread-safe operations

**Worker Pool:**
- âœ… Configurable concurrency (WORKER_CONCURRENCY env var)
- âœ… Per-job timeout enforcement (JOB_TIMEOUT env var)
- âœ… Graceful shutdown (waits for running jobs)
- âœ… Automatic job completion/failure tracking
- âœ… Context cancellation support
- âœ… Worker-level error handling

**Observability:**
- âœ… Comprehensive logging for all operations
- âœ… Job execution timing
- âœ… Retry attempt tracking

## Architecture Overview

### Components

- **API Server** (`cmd/api`): HTTP API for job submission and status queries
- **Worker** (`cmd/worker`): Processes jobs from Redis queue with configurable concurrency and graceful shutdown
- **Scheduler** (`cmd/scheduler`): Moves scheduled jobs to ready queues when their retry time arrives (runs every second)

### Worker Pool Architecture

The worker service uses a **concurrent worker pool** pattern:

1. **Pool Manager** (`internal/worker/pool.go`):
   - Manages N worker goroutines (configurable via `WORKER_CONCURRENCY`)
   - Each worker continuously dequeues jobs from Redis
   - Distributes work across workers for parallel processing
   - Implements graceful shutdown (waits for in-flight jobs)

2. **Executor** (`internal/worker/executor.go`):
   - Executes individual jobs by looking up registered handlers
   - Integrates with Redis queue for completion/failure tracking
   - Calls `queue.Complete()` on success
   - Calls `queue.Fail()` on error (triggers exponential backoff)
   - Enforces per-job timeout from configuration

3. **Handler Registry** (`internal/worker/handler.go`):
   - Maps job names to handler functions
   - Allows dynamic handler registration
   - Type-safe handler interface

**Example Worker Flow:**
```
Worker 1 â†’ Dequeue â†’ Execute â†’ Complete/Fail â†’ Dequeue â†’ ...
Worker 2 â†’ Dequeue â†’ Execute â†’ Complete/Fail â†’ Dequeue â†’ ...
Worker N â†’ Dequeue â†’ Execute â†’ Complete/Fail â†’ Dequeue â†’ ...
```

### Internal Packages

- `internal/queue`: Queue management and Redis operations
- `internal/job`: Job definition and state management
- `internal/config`: Configuration management
- `internal/worker`: Worker pool, executor, and task execution logic

### Client Package

- `pkg/client`: Go client library for interacting with the Bananas API

## Project Structure

```
.
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ api/              # API server entry point
â”‚   â”œâ”€â”€ worker/           # Worker entry point
â”‚   â””â”€â”€ scheduler/        # Scheduler entry point
â”œâ”€â”€ internal/             # Private application code
â”‚   â”œâ”€â”€ queue/            # Queue implementation
â”‚   â”œâ”€â”€ job/              # Job types and management
â”‚   â”œâ”€â”€ config/           # Configuration
â”‚   â””â”€â”€ worker/           # Worker logic
â”œâ”€â”€ pkg/                  # Public libraries
â”‚   â””â”€â”€ client/           # Client SDK
â”œâ”€â”€ docker/               # Docker configuration
â”‚   â”œâ”€â”€ Dockerfile.api        # Production build for API
â”‚   â”œâ”€â”€ Dockerfile.worker     # Production build for worker
â”‚   â”œâ”€â”€ Dockerfile.scheduler  # Production build for scheduler
â”‚   â”œâ”€â”€ Dockerfile.dev        # Development build with hot reload
â”‚   â”œâ”€â”€ .air.api.toml         # Hot reload config for API
â”‚   â”œâ”€â”€ .air.worker.toml      # Hot reload config for worker
â”‚   â””â”€â”€ .air.scheduler.toml   # Hot reload config for scheduler
â”œâ”€â”€ docker-compose.yml    # Production orchestration
â”œâ”€â”€ docker-compose.dev.yml # Development orchestration with hot reload
â”œâ”€â”€ Makefile              # Convenient command aliases
â”œâ”€â”€ .dockerignore         # Docker build exclusions
â”œâ”€â”€ .gitignore            # Git exclusions
â”œâ”€â”€ go.mod                # Go module definition
â””â”€â”€ README.md             # Project documentation
```

### Docker Configuration

**Production (`docker-compose.yml`)**:
- Multi-stage builds for minimal image size
- **Build stage**: `golang:1.23-alpine` for compiling
- **Runtime stage**: `alpine:latest` (small, optimized)
- Optimized binaries with no development tools

**Development (`docker-compose.dev.yml`)**:
- Single-stage build with `golang:1.23-alpine`
- Includes [Air](https://github.com/cosmtrek/air) for hot reload
- Source code mounted as volume
- Instant feedback on code changes (~1s restart)
- No image rebuilds needed during development

Both setups include:
- Health checks for service dependencies
- Restart policies for reliability
- Environment variables for configuration

## License

MIT
